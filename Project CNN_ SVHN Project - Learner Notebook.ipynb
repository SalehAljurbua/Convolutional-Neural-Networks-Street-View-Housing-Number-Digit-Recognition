{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q91KqmCRu64D"
   },
   "source": [
    "# Project - Convolutional Neural Networks: Street View Housing Number Digit Recognition\n",
    "\n",
    "**Marks: 30**\n",
    "\n",
    "Dear Learner,\n",
    "\n",
    "Welcome to project on Classification using Convolutional Neural Networks. We will work with the same dataset that we used for the Neural Network project: Street View Housing Numbers image dataset.\n",
    "\n",
    "Do read the problem statement and the guidelines around the same.\n",
    "\n",
    "----\n",
    "### Context: \n",
    "-------\n",
    "\n",
    "The ability to process visual information using machine learning algorithms can be very useful as demonstrated in various applications. The Street View House Numbers (SVHN) dataset is one of the most popular ones. It has been used in neural networks created by Google to read house numbers and match them to their geolocations. This is a great benchmark dataset to play with, learn and train models that accurately identify street numbers, and incorporate into all sorts of projects.\n",
    "\n",
    "---------\n",
    "### Objective:\n",
    "------------\n",
    "The objective of the exercise is to perform an image classification exercise on the given dataset to come up with a model that can help identify the digit images which have issues like picture brightness, blurriness. \n",
    "\n",
    "--------\n",
    "### More about the dataset\n",
    "------------\n",
    "- The dataset is provided as a .h5 file. The basic preprocessing steps have been done.\n",
    "\n",
    "---------------------------\n",
    "### Guidelines\n",
    "-----------------------------------------\n",
    "- Note that some of the questions are similar to the ones asked in the previous project. Semi filled codes are not provided in those cases.\n",
    "- You need to download the dataset from the given link and add it to your drive. Use colab for this exercise. \n",
    "- You will need to mount the drive and give proper path to read the dataset.\n",
    "- The exercise consists of semi written code blocks. You need to fill the blocks as per the instructions to achieve the required results.\n",
    "- To be able to complete the assessment in the expected time, do not change the variable names. The codes might throw errors when the names are changed. \n",
    "- The marks of each requirement is mentioned in the question.\n",
    "- You can raise your issues on the discussion forum on the Olympus.\n",
    "- Uncomment the code snippets and work on them\n",
    "--------------------------------------------\n",
    "Wishing you all the best!\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8z2Z7-OAs8QG"
   },
   "source": [
    "### Mount the drive\n",
    "Let us start by mounting the drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27664,
     "status": "ok",
     "timestamp": 1606918627073,
     "user": {
      "displayName": "Shrish Chandra Pandey",
      "photoUrl": "",
      "userId": "03777729159992968707"
     },
     "user_tz": -330
    },
    "id": "REFUdThmpz_d",
    "outputId": "b809ed14-82bc-4287-c842-1752880f13cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ucnevGLoyKf_"
   },
   "source": [
    "Let us check for the version of installed tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27654,
     "status": "ok",
     "timestamp": 1606918627074,
     "user": {
      "displayName": "Shrish Chandra Pandey",
      "photoUrl": "",
      "userId": "03777729159992968707"
     },
     "user_tz": -330
    },
    "id": "W5as47YxyJVk",
    "outputId": "eba080be-290a-4465-c688-ce9e34f4266e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8lsux2ZwyTTR"
   },
   "source": [
    "### Load the dataset\n",
    "- Let us now, load the dataset that is available as a .h5 file.\n",
    "- Split the data into train and the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BApX9qgNsqV0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "# Open the file as readonly\n",
    "# Make changes in path as required\n",
    "h5f = h5py.File('/content/drive/My Drive/SVHN_single_grey1.h5', 'r')\n",
    "\n",
    "# Load the training and the test set\n",
    "X_train = h5f['X_train'][:]\n",
    "y_train1 = h5f['y_train'][:]\n",
    "X_test = h5f['X_test'][:]\n",
    "y_test1 = h5f['y_test'][:]\n",
    "\n",
    "\n",
    "# Close this file\n",
    "h5f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nX4VEpfIyeaW"
   },
   "source": [
    "Let us import the required libraries now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aTeAO8HpsqV8"
   },
   "outputs": [],
   "source": [
    "## Importing the required libraries\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, BatchNormalization, Dropout, Flatten, LeakyReLU\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kxODV6HKykuc"
   },
   "source": [
    "### Visualising images (1.5 marks)\n",
    "- Use X_train to visualise the first 10 images. (1 marks)\n",
    "- Use Y_train to print the first 10 labels (0.5 mark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bvsc8ytHsqWD"
   },
   "outputs": [],
   "source": [
    "# visualizing the first 10 images in the dataset and their labels\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kzoyeXHOy80N"
   },
   "source": [
    "### Data preparation (6 marks)\n",
    "\n",
    "- Print the first image in the train image and figure out the shape of the images (0.5 mark)\n",
    "- Reshape the train and the test dataset to make them fit the first convolutional operation that we will create later. Figure out the required shape (3 marks)\n",
    "- Normalise the train and the test dataset by dividing by 255. (1 mark)\n",
    "- Print the new shapes of the train and the test set. (0.5 mark)\n",
    "- One hot encode the target variables (1 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NqndzQXng9rL"
   },
   "outputs": [],
   "source": [
    "# Shape of the images and the first image\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D9YPwf9ysqWU"
   },
   "outputs": [],
   "source": [
    "# Reshaping the dataset to flatten them. Remember that we are trying to reshape the 2D image data into a 3D data where there is just one channel\n",
    "\n",
    "#Uncomment below to answer\n",
    "# x_train = X_train.reshape(__________)\n",
    "# x_test = X_test.reshape(___________)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q_yUUTp_mUzB"
   },
   "outputs": [],
   "source": [
    "# Normalize inputs from 0-255 to 0-1\n",
    "\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t7FSqOpamWkH"
   },
   "outputs": [],
   "source": [
    "# New shape \n",
    "\n",
    "#Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zL0lYER4sqWw"
   },
   "outputs": [],
   "source": [
    "# one hot encode output\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UJDUoaEj1d6e"
   },
   "source": [
    "### Model Building (7 marks)\n",
    "- Write a function that returns a sequential model with the following architecture\n",
    " - First Convolutional layer with 16 filters and kernel size =3. Use the 'same' padding and provide apt input shape.\n",
    " - Add a leaky relu layer next with the value 0.1\n",
    " - First Convolutional layer with 32 filters and kernel size =3. Use the 'same' padding.\n",
    " - Another leakyRelu same as above.\n",
    " - A maxpooling layer with pool size of 2\n",
    " - Flatten the output from the previous layer\n",
    " - Add a dense layer with 32 nodes\n",
    " - Add a leakyRelu layer with slope(0.1)\n",
    " - Add the final output layer with nodes equal to the number of classes and softmax activation.\n",
    " - Compile the model with the categorical_crossentropy loss, adam optmizers (lr = 0.001) and accuracy metric.\n",
    "- Do not fit the model here, just return the compiled model\n",
    "- Call the model and print the model summary\n",
    "- Fit the model on the train data with a validation split of 0.2, batch size = 32, verbose = 1 and 20 epochs. Store the model building history to use it later for visualisation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cmi81Gr5sqW-"
   },
   "outputs": [],
   "source": [
    "# define model\n",
    "\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "# Uncomment below to answer\n",
    "\n",
    "# def cnn_model_1():\n",
    "#     model_1 = Sequential()\n",
    "#     #Your code here\n",
    "#     return model_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tiOOOyHkhQH_"
   },
   "outputs": [],
   "source": [
    "# Call the function and print the model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bGCUI_xsImnH"
   },
   "outputs": [],
   "source": [
    "# Fit the model and save the history\n",
    "# Uncomment below to answer\n",
    "\n",
    "# history_model_1 = model_1.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tKOckG-KPyLg"
   },
   "source": [
    "### Plotting the validation and training accuracies (1.5 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lt77zgGMP4yw"
   },
   "outputs": [],
   "source": [
    "# plotting the accuracies\n",
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pGBbQpLONX7k"
   },
   "source": [
    "**Comments**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jW-NhoXXm2b9"
   },
   "source": [
    "### Iteration 2 (12 marks)\n",
    "- Experiment with adding dropout layers to make the model generalise better and report the results.\n",
    "- Feel free to explore various architectures that can help you generalise better.\n",
    "- Repeat all the steps done above and plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wQtMWG4znKS0"
   },
   "outputs": [],
   "source": [
    "# Uncomment below and complete\n",
    "\n",
    "# def cnn_model_2():\n",
    "#     # initialized a sequential model\n",
    "#     model_2 = Sequential()\n",
    "#     # Your code here\n",
    "#     return model_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cu2ydBfHnlx1"
   },
   "outputs": [],
   "source": [
    "#Call the function and print model summary\n",
    "\n",
    "#Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3x_GZq2-n-QO"
   },
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "# Uncomment below and complete\n",
    "# history_model_2 = model_2.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RjYchTv2pnsW"
   },
   "outputs": [],
   "source": [
    "# plotting the accuracies\n",
    "\n",
    "#Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HWr-tucap_TE"
   },
   "source": [
    "#### Comments:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8kuXx9Bvu00f"
   },
   "source": [
    "### Test set prediction and final comments (Using the better model of the two iterations) (2 marks)\n",
    "- predict on the test set and comment on the resultls obtained. (2 marks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xRddeJ-3EHT1"
   },
   "outputs": [],
   "source": [
    "# predict on the test dataset\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DjErl4GA2u9s"
   },
   "source": [
    "#### Comments\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e5ZGAuNBqznd"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "UJDUoaEj1d6e",
    "8kuXx9Bvu00f"
   ],
   "name": "Project CNN: SVHN Project - Learner Notebook.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
